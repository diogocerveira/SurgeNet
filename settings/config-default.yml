# General
seed: 53  # 5 --> 3 or 3 --> 5
device: None
path_root: .
action: train # process (sample, modeFitler)
                # train (spatial, temporal)
                # eval (aprfc, phaseTiming, phaseChart)

# Data
datakey: local  # match with datasetName (local for LDSS, external for CIFAR10)
datasetName: LDSS-lite
preprocessing: basic  # basic, augmentation
datapointExtras: # for local dataset (extend __getitem__() return beyond datapoint and label)
  id: True

# Model
modelkey: spatial # spatial, temporal
fx_weights: default
fx_model: LDSS-20241104-00--0.837800.pth
fx_batchSize: 128  # training feature extractor
n_stages_G: 1  # G for Guesser
n_stages_R: 2  # R for Refiner
n_filters: 64  # number of convolution filters == number of channels inside a stage
filterTime: 3  # size in time
n_resBlocks_G: 2  # inside the stage
n_resBlocks_R: 2

# Process
processkey: modeFilter  # (sample, fextract, modeFilter)

# Train
trainkey: start # (start, resume)
train_outputs:
  predictions: True
  features: True
  checkpoints: True
  events: True
  bestModel: True
k_folds: 7 # match with datasetName (7 for LDSS, 5 for CIFAR10)
n_epochs: 10
batchSize: 128
criterion: crossEntropy
train_metric: accuracy
valid_metric: accuracy
learningRate: 0.0005
momentum: 0.9
gamma: 0.1
stepSize: 30  # optimizer scheduler (to change mid train)
path_resume: null  # to continue training from a checkpoint model
checkpointFrequency: 5 # epoch frequency to save a checkpoint
skipFolds: []  # 0 for fold 1...

# Eval
evalkeys:
  acpref1coma: False
  phaseTiming: False
  phaseChart: False
agg: macro  # accuracy is locked to micro (see utils.py)
updateFrequency: 1  # updates metrics with new data every batch
computeFrequency: 100  # computes the metrics
metricSwitches:
  accuracy: True
  precision: True
  recall: True
  f1score: True
  confusionMatrix: True