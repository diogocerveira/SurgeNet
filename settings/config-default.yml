GENERAL:
  seed: 53  # 5 --> 3 or 3 --> 5
  device: None
  path_root: .
  load_run:  # if empty creates new run, else loads it (YYMM-DD-HH)
  actions:
  - train        # train (spatial, temporal, full)
  - process      # process (sample-n-label, mode-filter, extractFeatures)
  - eval         # eval (aprfc, phaseTiming, phaseChart)

DATA:
  datasetId: LDSSlite-local  # (ldss-local, cifar10-ext)
  preprocessing: basic  # basic, augmentation
  return_extradata: # for local dataset (extends __getitem__() return beyond datapoint and label)
    frameId: True
    videoId: False
    timestamp: False
  path_rawVideos: 

PROCESS:  # Process
  sample: False
  label: False
  update_labels: False
  samplerate: 1 # Hz
  filter_annotated: True
  apply_modeFilter: True
  path_bundle: True
  export_modedPreds: True  # if apply-mode-filter == True
  fx_spatial: True

MODEL:  # for models created/trained in the run 
  domain: spatial # spatial, temporal, full
# spatial
  spaceWeights: default # sfx = spatial feature extraction
  spaceArch: resnet50
  n_classes: 6  # normally get from dataset itself
  path_spaceModel: #C:/Users/DC/Documents/Programming/python/ucl/ML_SurgeNet/logs/LDSS/run_20241104-00/checkpoints/LDSS-20241104-00-fold6-0.837800.pth # if empty, uses new untrained one
# temporal
  n_stages: 3
  n_filters: 64  # number of convolution filters == number of channels inside a stage
  tf_filter: 3  # size in time (timeframe, # of frames)
  n_blocks: 2  # inside the stage

TRAIN:
  train_point: start # (start, resume)
  path_resumeModel: None  # to continue training from a checkpoint model if train-point == resume
  save_checkpoints: True
  checkpointRate: 5 # epoch frequency to save a checkpoint
  save_betterModel: True
  log_events: True
  criterionId: weighted-cross-entropy
  train_metric: accuracy
  valid_metric: accuracy
  k_folds: 7 # match with datasetName (7 for LDSS, 5 for CIFAR10, 1 for None?)
  skipFolds: []  # 0 for fold 1... after a break (e.g. crash) allows to skip folds and restart from a checkpoint
  HYPER:  # traditional hyperparameters
    n_epochs: 50
    batchSize: 128
    learningRate: 0.0005  # learning rate
    momentum: 0.9
    gamma: 0.1
    stepSize: 30  # optimizer scheduler (to change mid train)
 

EVAL:  # Eval
  eval_from: predictions  # model
  path_model:  # if eval-from == model and no training done
  path_preds:
  aprfc: False
  phaseTiming: False
  phaseChart: False
  export_testBundle: True
# metrics
  test_metrics:
    accuracy: True
    precision: True
    recall: True
    f1score: True
    confusionMatrix: True
  agg: macro  # overall, accuracy is locked to micro (see utils.py)
  updateRate: 1  # updates metrics with new data every batch
  computeRate: 100  # computes the metrics